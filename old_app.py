import logging
from flask import Flask, request, jsonify
from azure.communication.callautomation import CallAutomationClient, FileSource, _models, RecognizeInputType
import azure.cognitiveservices.speech as speechsdk
from azure.storage.blob import BlobServiceClient
from twilio.rest import Client
from pydub import AudioSegment
import requests
import os
import time
import wave
import io
import pyaudio
import numpy as np
import pyaudio
import wave
import numpy as np
import time
import json

app = Flask(__name__)

# Configuration Azure Communication Service
call_automation_client = CallAutomationClient.from_connection_string("endpoint=https://lyraetalk.france.communication.azure.com/;accesskey=3w3cK83UG45fDt4zOVi4mwSsApOvCbqfZhn1tKFn4TPMp5d5umCYJQQJ99ALACULyCpuAreVAAAAAZCS6qkJ")

# Configuration Azure Blob Storage
blob_service_client = BlobServiceClient.from_connection_string("DefaultEndpointsProtocol=https;AccountName=lyraetalk8b7e;AccountKey=zOX5Zz9YrLI4d9bUz0qg8HQFYKvaHL1wGF31BOUzrdGgl7YNjsrZ43ZkvvkvksDAQaIWLcbcvoKp+AStnfIECA==;EndpointSuffix=core.windows.net")
container_name = "recordings"

# Configuration Twilio
twilio_client = Client("AC018cf7591ccc8dc367e58896e52ca76e", "192937914f8c996785bb6ac58c014f5b")

# Configuration Azure TTS
speech_config = speechsdk.SpeechConfig(subscription="CwdBzhR9vodZ5lXf4S52ErZaUy9eUG05JJCtDuu4xjjL5rylozVFJQQJ99BAAC5T7U2XJ3w3AAAAACOGuWEK", region="francecentral")
speech_config.speech_synthesis_voice_name = "fr-FR-DeniseNeural"
is_ia_speaking = False
first_callback_done = False

@app.route("/recording_callback", methods=["POST"])
def recording_callback():
    data = request.json[0]
    event_type = data.get("type")
    print(data)
    if event_type == "Microsoft.Communication.RecordingFileStatusUpdated":
        recording_status = data.get("data").get("recordingStatus")
        if recording_status == "available":
            recording_id = data.get("data").get("recordingId")
            print(f"Enregistrement disponible. ID : {recording_id}")
            # T√©l√©charger le fichier .wav depuis Azure Blob Storage
            download_audio_file(recording_id)
    
    return jsonify({"status": "success"})

@app.route("/callback", methods=["POST"])
def callback():
    global first_callback_done
    print(request.json[0].get("type"))
    data = request.json[0]
    if request.json and request.json[0].get("type") == "Microsoft.Communication.PlayCompleted" and first_callback_done == False:
        first_callback_done = True
        call_connection_id = data.get("data").get("callConnectionId")
        audio_stream = capture_audio_stream()
        response_file = f"response_{call_connection_id}_{int(time.time())}.wav"
        record_response(audio_stream, response_file)

        user_response = transcribe_audio(response_file, call_connection_id)
        if user_response == 0:
            while user_response == 0:
                global is_ia_speaking
                is_ia_speaking = False
                speak("D√©sol√©, je n'ai pas entendu, pouvez-vous r√©p√©ter ?", call_connection_id)
                print("STARTING NEW RECORDING")
                new_audio_stream = capture_audio_stream()
                response_file = f"response_{call_connection_id}_{int(time.time())}.wav"
                record_response(new_audio_stream, response_file)
                user_response = transcribe_audio(response_file, call_connection_id)
            
        print(f"R√©ponse de l'appelant : {user_response}")

        intent = get_intent(user_response)
        print("INTENT", intent)
        if intent == "gestion":
            handle_gestion(user_response)
        elif intent == "renseignement":
            handle_renseignement(user_response, call_connection_id)
        # # Envoyer la r√©ponse au mod√®le
    if request.json and request.json[0].get("type") == "Microsoft.Communication.PlayFailed":
        print(request.json[0])
    if request.json and request.json[0].get("type") == "Microsoft.Communication.CallConnected":
        call_connection_id = data.get("data").get("callConnectionId")
        server_call_id = data.get("data").get("serverCallId")
        caller = request.args.get('caller')
        start_recording(server_call_id=server_call_id, call_connection_id=call_connection_id, caller=caller)
        # start_conversation(server_call_id, call_connection_id)
    return jsonify({"status": "success"})

@app.route("/test", methods=["POST"])
def test():
    print("test")
    return(jsonify({"status": "success"}))

@app.route("/incoming_call", methods=["POST"])
def incoming_call():
    if request.json and request.json[0].get("eventType") == "Microsoft.EventGrid.SubscriptionValidationEvent":
        validation_code = request.json[0]["data"]["validationCode"]
        return jsonify({"validationResponse": validation_code}), 200

    data = request.json[0]
    caller = data.get("data").get("from").get("rawId")
    encodedContext = data.get("data").get("incomingCallContext")
    media_streaming_options = _models.MediaStreamingOptions(
        transport_url="ws://40.66.34.174:3000",
        transport_type="websocket",
        content_type="audio",
        audio_channel_type="unmixed",
        start_media_streaming=False,
        enable_bidirectional=False,
        audio_format="Pcm16KMono"
    )
    call_automation_client.answer_call(incoming_call_context=encodedContext, callback_url=f"https://b263-2a01-cb00-844-1d00-91d7-c8f2-e398-1263.ngrok-free.app/callback?caller={caller}", media_streaming=media_streaming_options)
    return jsonify({"status": "success"})

from pydub import AudioSegment
import time

def speak(text, call_connection_id):
    """
    Utilise Azure Text-to-Speech (TTS) pour dire √† voix haute le texte et joue l'audio dans l'appel.
    Attend que la lecture audio soit termin√©e avant de continuer.

    Args:
        text (str): Le texte √† convertir en parole.
        call_connection_id (str): L'ID de la connexion d'appel.
    """
    global is_ia_speaking
    is_ia_speaking = True

    # Configuration du service TTS
    speech_config = speechsdk.SpeechConfig(
        subscription="CwdBzhR9vodZ5lXf4S52ErZaUy9eUG05JJCtDuu4xjjL5rylozVFJQQJ99BAAC5T7U2XJ3w3AAAAACOGuWEK",  # Remplacez par votre cl√© d'abonnement
        region="francecentral"  # Remplacez par votre r√©gion
    )
    speech_config.speech_synthesis_voice_name = "fr-FR-DeniseNeural"  # Choisissez une voix fran√ßaise

    # Cr√©er un fichier audio temporaire
    temp_audio_file = f"temp_tts_output_{int(time.time())}.wav"
    audio_config = speechsdk.audio.AudioOutputConfig(filename=temp_audio_file)

    # Cr√©er un synthesizer TTS
    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)

    # Convertir le texte en parole
    print(f"Conversion du texte en parole : {text}")
    result = speech_synthesizer.speak_text_async(text).get()

    # V√©rifier si la synth√®se a r√©ussi
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        print("Synth√®se audio termin√©e.")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        print(f"Synth√®se audio annul√©e : {cancellation_details.reason}")
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
            print(f"Erreur : {cancellation_details.error_details}")
        return

    # R√©cup√©rer la dur√©e du fichier audio
    audio = AudioSegment.from_file(temp_audio_file)
    duration_in_seconds = len(audio) / 1000  # Convertir la dur√©e en secondes
    print(f"Dur√©e du fichier audio : {duration_in_seconds} secondes")

    # T√©l√©verser le fichier audio vers Azure Blob Storage
    blob_service_client = BlobServiceClient.from_connection_string("DefaultEndpointsProtocol=https;AccountName=lyraetalk8b7e;AccountKey=zOX5Zz9YrLI4d9bUz0qg8HQFYKvaHL1wGF31BOUzrdGgl7YNjsrZ43ZkvvkvksDAQaIWLcbcvoKp+AStnfIECA==;EndpointSuffix=core.windows.net")
    container_name = "recordings"
    blob_name = f"tts_output_{call_connection_id}.wav"

    with open(temp_audio_file, "rb") as audio_file:
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
        blob_client.upload_blob(audio_file, overwrite=True)

    # G√©n√©rer l'URL du fichier audio
    audio_url = f"https://lyraetalk8b7e.blob.core.windows.net/recordings/{blob_name}"

    # Jouer l'audio dans l'appel
    play_source = FileSource(url=audio_url)
    call_automation_client.get_call_connection(call_connection_id).play_media_to_all(play_source=play_source)

    # Attendre que la dur√©e du fichier audio soit √©coul√©e
    startTime = time.time()
    print(f"Attente de {duration_in_seconds} secondes pour la fin de la lecture...")
    time.sleep(duration_in_seconds + 1)
    endTime = time.time()

    print("TTS message jou√© dans l'appel.")

def speak_hello(text, call_connection_id):
    audio_url = f"https://lyraetalk8b7e.blob.core.windows.net/recordings/bonjour_message.wav"
    play_source = FileSource(url=audio_url)
    # Play the audio in the call
    call_automation_client.get_call_connection(call_connection_id).play_media_to_all(play_source=play_source)

    print("TTS message played in the call.")

# def start_recording(server_call_id, call_connection_id):
#     print("CALL_CONNECTION_ID", call_connection_id)
#     recording_state = call_automation_client.start_recording(call_locator=server_call_id)
#     return recording_state.recording_id

def is_silence(pcm_data):
    audio_samples = np.frombuffer(pcm_data, dtype=np.int16)
    rms = np.sqrt(np.mean(np.square(audio_samples)))  # Calcul du RMS
    return rms < 500  # True si silencieux

def start_recording(server_call_id, call_connection_id, caller):
    # try:
    print("start streaming")

    test = call_automation_client.get_call_connection(call_connection_id).start_media_streaming(
        operation_context="startMediaStreamingContext",
    )

    print(test)
    silence_counter = 0
    recording = True

    while recording:
        if not pcm_data:
            continue

        if is_silence(pcm_data):
            silence_counter += 1
            if silence_counter >= 1 * 10:  # 10 it√©rations par seconde
                print("üõë Silence d√©tect√©, arr√™t de l'enregistrement")
                recording = False
        else:
            silence_counter = 0  # Reset du compteur si l‚Äôutilisateur parle

        time.sleep(0.1)  # Pause courte pour √©viter une boucle trop rapide
    time.sleep(3)

    call_automation_client.get_call_connection(call_connection_id).stop_media_streaming(operation_context="stopMediaStreamingContext")
    # test = call_connection.start_recognizing_media(
    #     input_type="speech",
    #     target_participant=PhoneNumberIdentifier(value=caller),
    #     initial_silence_timeout=5,
    #     speech_language="fr-FR"
    # )
    # print(test)
    #     recording_state = call_automation_client.start_recording(
    #         call_locator=_models.ServerCallLocator(server_call_id),
    #         recording_content_type="audio",
    #         recording_channel_type="mixed",
    #         recording_format_type="wav",
    #         recording_storage=_models.AzureBlobContainerRecordingStorage(container_url="https://lyrae8b7e.blob.core.windows.net/recordings")
    #     )
    #     time.sleep(2)
    #     call_automation_client.stop_recording(recording_id=recording_state.recording_id)
    # except Exception as e:
    #     print(f"An error occurred: {e}")
    
    # print(f"Recording State: {recording_state.recording_state}")

    # speech_config.request_word_level_timestamps()
    # speech_config.set_property(
    #     property_id=speechsdk.PropertyId.SpeechServiceResponse_OutputFormatOption, value="detailed")

    # # Creates a speech recognizer using the default microphone (built-in).
    # audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)

    # speech_recognizer = speechsdk.SpeechRecognizer(
    #     speech_config=speech_config, audio_config=audio_config)

    # results = []    
    # done = False

    # def speech_detected():
    #     nonlocal lastSpoken
    #     lastSpoken = int(datetime.now().timestamp() * 1000)

    # def handleResult(evt):
    #     import json
    #     nonlocal results
    #     nonlocal lastSpoken
    #     results.append(json.loads(evt.result.json))

    #     # print the result (optional, otherwise it can run for a few minutes without output)
    #     # print('RECOGNIZED: {}'.format(evt))
    #     speech_detected()

    #     # result object
    #     res = {'text': evt.result.test, 'timestamp': evt.result.offset,
    #            'duration': evt.result.duration, 'raw': evt.result}

    #     if (evt.result.text != ""):
    #         results.append(res)

    # def stop_cb(evt):
    #     # print('CLOSING on {}'.format(evt))
    #     speech_recognizer.stop_continuous_recognition()
    #     nonlocal done
    #     done = True

    # speech_recognizer.recognizing.connect(lambda evt: speech_detected())
    # speech_recognizer.session_started.connect(
    #     lambda evt: print('SESSION STARTED: {}'.format(evt)))
    # speech_recognizer.session_stopped.connect(
    #     lambda evt: print('SESSION STOPPED {}'.format(evt)))
    # speech_recognizer.canceled.connect(
    #     lambda evt: print('CANCELED {}'.format(evt)))
    # speech_recognizer.recognized.connect(handleResult)
    # speech_recognizer.session_stopped.connect(stop_cb)
    # speech_recognizer.canceled.connect(stop_cb)

    # lastSpoken = int(datetime.now().timestamp() * 1000)

    # while not done:
    #     time.sleep(1)
    #     now = int(datetime.now().timestamp() * 1000)
    #     inactivity = now - lastSpoken
    #     # print(inactivity)
    #     # After 1 second of no speech detected, play a sound to indicate the recoding session could close.
    #     if (inactivity > 1000):
    #         print("inactive")
    #         # play_sound()
    #     if (inactivity > 3000):  # Close the recoding session if no input is detected after 3s
    #         print('Stopping async recognition.')
    #         speech_recognizer.stop_continuous_recognition_async()
    #         speak("Thank you!")
    #         while not done:
    #             time.sleep(1)

def stop_recording(recording_id):
    call_automation_client.stop_recording(recording_id)

def download_audio_file(recording_id):
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=f"{recording_id}.wav")
    with open(f"{recording_id}.wav", "wb") as audio_file:
        print(blob_client.download_blob())
        audio_file.write(blob_client.download_blob().readall())
    return f"{recording_id}.wav"

def get_intent(user_response):
    print("RETRIEVING INTENT")
    url = "https://lyraetalk-detection-intention-patient-info-rdv.azurewebsites.net/api/detect_intent?code=IE1hYj0VJKw7J3jICYLTfUx7CmOKChjp8xJmqBS6nKi7AzFu4oV75w=="
    headers = {
        "Content-Type": "application/json"
    }
    payload = {"text": user_response}
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()  # L√®ve une exception si le statut HTTP n'est pas 200
        print(response.json())
        return response.json().get("response", "Pas de r√©ponse trouv√©e.")
    except requests.exceptions.RequestException as e:
        print(f"Erreur lors de l'appel au mod√®le : {e}")
        return "Erreur lors de la communication avec le mod√®le."

def get_model_response(text):
    global is_ia_speaking
    print("SENDING TEXT", text)
    url = "https://medical-rad-rag-assistant.azurewebsites.net/api/rag_query?code=MjVVHBDAeLnYyXz0FzwYsaGxSjFXT99s4vaQg_nUlKe9AzFuuU3Z4Q=="
    headers = {
        "Content-Type": "application/json"
    }
    payload = {"text": text}
    is_ia_speaking=False
    try:
        print("STARTING MODEL REQUEST")
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()  # L√®ve une exception si le statut HTTP n'est pas 200
        return response.json().get("response", "Pas de r√©ponse trouv√©e.")
    except requests.exceptions.RequestException as e:
        print(f"Erreur lors de l'appel au mod√®le : {e}")
        return "Erreur lors de la communication avec le mod√®le."

def record_response(audio_stream, file_name):
    with wave.open(file_name, 'wb') as wav_file:
        wav_file.setnchannels(1)  # Mono
        wav_file.setsampwidth(2)  # 16 bits
        wav_file.setframerate(16000)  # 16 kHz
        wav_file.writeframes(audio_stream.read())

def upload_to_blob_storage(local_file_path, file_name):
    """ Uploads the file to Azure Blob Storage and returns the public URL """
    container_name = "recordings"
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)

    with open(local_file_path, "rb") as data:
        blob_client.upload_blob(data, overwrite=True)

    return f"https://lyraetalk8b7e.blob.core.windows.net/recordings/bonjour_message.wav"

def capture_audio_stream(silence_threshold=200, silence_duration=2, sample_rate=16000, channels=1, chunk_size=512):
    """
    Capture l'audio en temps r√©el et s'arr√™te apr√®s 1 seconde de silence.
    
    Args:
        silence_threshold (int): Seuil de silence (amplitude en dessous de laquelle on consid√®re qu'il y a silence).
        silence_duration (float): Dur√©e de silence (en secondes) avant d'arr√™ter l'enregistrement.
        sample_rate (int): Taux d'√©chantillonnage (par d√©faut 16 kHz).
        channels (int): Nombre de canaux audio (1 pour mono, 2 pour st√©r√©o).
        chunk_size (int): Taille des blocs audio √† lire.
    
    Returns:
        io.BytesIO: Flux audio en m√©moire.
    """
    global is_ia_speaking

    # Attendre que l'IA ait fini de parler
    while is_ia_speaking:
        time.sleep(0.1)

    # Initialiser PyAudio
    audio = pyaudio.PyAudio()

    # Ouvrir le flux audio
    stream = audio.open(
        format=pyaudio.paInt16,  # Format 16 bits
        channels=channels,       # Mono
        rate=sample_rate,        # Taux d'√©chantillonnage
        input=True,              # Mode entr√©e (microphone)
        frames_per_buffer=chunk_size
    )

    print("D√©but de l'enregistrement...")

    # Variables pour la d√©tection de silence
    frames = []
    silent_chunks = 0
    silence_limit = int(silence_duration * sample_rate / chunk_size)  # Nombre de chunks silencieux pour 1 seconde

    while True:
        # Lire un chunk audio
        data = stream.read(chunk_size)
        frames.append(data)

        # Convertir les donn√©es audio en tableau numpy pour analyser l'amplitude
        audio_data = np.frombuffer(data, dtype=np.int16)
        amplitude = np.abs(audio_data).mean()

        # D√©tecter le silence
        if amplitude < silence_threshold:
            silent_chunks += 1
        else:
            silent_chunks = 0

        # Arr√™ter l'enregistrement apr√®s 1 seconde de silence
        if silent_chunks > silence_limit:
            print("Silence d√©tect√©, fin de l'enregistrement.")
            break

    # Arr√™ter et fermer le flux
    stream.stop_stream()
    stream.close()
    audio.terminate()

    # Cr√©er un flux en m√©moire avec les donn√©es audio
    audio_stream = io.BytesIO()
    with wave.open(audio_stream, 'wb') as wav_file:
        wav_file.setnchannels(channels)
        wav_file.setsampwidth(audio.get_sample_size(pyaudio.paInt16))
        wav_file.setframerate(sample_rate)
        wav_file.writeframes(b''.join(frames))

    # Remettre le pointeur du flux au d√©but pour une lecture ult√©rieure
    audio_stream.seek(0)

    return audio_stream

def handle_gestion(user_response):
    
    return "ok"

def handle_renseignement(user_response, call_connection_id):
    print("CALLING MODEL")
    model_response = get_model_response(user_response)
    print(model_response)
    speak(model_response, call_connection_id=call_connection_id)
    return "ok"

def increase_volume(audio_file_path, target_dBFS=-20.0):
    """
    Augmente le volume d'un fichier audio en normalisant l'audio.

    Args:
        audio_file_path (str): Chemin du fichier audio √† modifier.
        target_dBFS (float): Volume cible en dBFS (par d√©faut -20 dBFS).

    Returns:
        str: Chemin du fichier audio modifi√©.
    """
    # Charger le fichier audio
    audio = AudioSegment.from_wav(audio_file_path)

    # Normaliser l'audio pour atteindre le volume cible
    normalized_audio = audio.apply_gain(target_dBFS - audio.dBFS)

    # Sauvegarder le fichier audio modifi√©
    output_file_path = audio_file_path.replace(".wav", "_louder.wav")
    normalized_audio.export(output_file_path, format="wav")

    return output_file_path

def transcribe_audio(audio_file_path, call_connection_id):
    print("CALL_CONNECTION_ID", call_connection_id)
    # Configuration de l'entr√©e audio (fichier WAV)

    louder_audio_file_path = increase_volume(audio_file_path)
    audio_input = speechsdk.audio.AudioConfig(filename=louder_audio_file_path)

    # Cr√©ation du recognizer
    speech_config.speech_recognition_language="fr-FR"  # Forcer la reconnaissance en fran√ßais
    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

    # D√©marrage de la transcription
    print("D√©but de la transcription...")
    result = speech_recognizer.recognize_once()

    # V√©rification du r√©sultat
    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        print("Transcription r√©ussie.")
        print(result.text)
        # speak("Avec plaisir, laissez-moi vous guider pas √† pas", call_connection_id=call_connection_id)
        return result.text
    elif result.reason == speechsdk.ResultReason.NoMatch:
        print("Aucune parole d√©tect√©e dans l'audio.")
        return 0
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        print(f"Transcription annul√©e : {cancellation_details.reason}")
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
            print(f"Erreur : {cancellation_details.error_details}")
        return ""

def start_conversation(server_call_id, call_connection_id):
    print("STARTING CONVERSATION")
    
    speech_config.speech_synthesis_voice_name = "fr-FR-DeniseNeural"  # French voice

    speak_hello("Bonjour, comment puis-je vous aider ?", call_connection_id=call_connection_id)
   
    # while True:
        # D√©tecter quand l'appelant commence √† parler
        # print("En attente de la r√©ponse de l'appelant...")

        # Transcrire la r√©ponse de l'appelant

        # # Demander si le besoin est r√©solu
        # speak("Avez-vous trouv√© la r√©ponse √† votre besoin ? R√©pondez par 'Oui' ou 'Non'.")

        # # Enregistrer la confirmation de l'appelant
        # audio_stream = capture_audio_stream()
        # confirmation_file = f"confirmation_{call_connection_id}_{int(time.time())}.wav"
        # record_response(audio_stream, confirmation_file)

        # # Transcrire la confirmation
        # confirmation = transcribe_audio(confirmation_file)
        # print(f"Confirmation de l'appelant : {confirmation}")

        # # Envoyer les fichiers vers Azure Blob Storage
        # upload_to_blob_storage(response_file)
        # upload_to_blob_storage(confirmation_file)

        # if "oui" in confirmation.lower():
        #     speak("Merci d'avoir appel√©. √Ä bient√¥t !")
        #     break
        # elif "non" in confirmation.lower():
        #     speak("Pouvez-vous reformuler votre demande ?")

if __name__ == "__main__":
    app.run(port=5000, debug=True)